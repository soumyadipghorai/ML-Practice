{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## MLP\n",
        "its a supervised algo. it learns a non-linear function approximator for either classification or regression.\n",
        "\n",
        "* MLPClassifier supports multiclass classification by applying softmax as the output function.\n",
        "* it also supports multilabel in which sample can belong to more than one class.\n",
        "* also supports multioutput regression where the sample can have more than one target.  \n",
        "\n",
        "MLPClassifier support only the cross entropy loss"
      ],
      "metadata": {
        "id": "Sfl9GDPOKl7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dQXHyXYRKjo2"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "MLP_clf = MLPClassifier()\n",
        "MLP_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLP_clf.predict(X_test) # gives labels for new samples\n",
        "MLP_clf.predict_proba(X_test) # gives vector of probability estimates per sample"
      ],
      "metadata": {
        "id": "Yq5YuqQ5LaI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to set the number of hidden layers?\n",
        "`hidden_layer_sizes` sets the number of layers and the number of neurons in each layer.\n",
        "* its a tuple where each element in the tuple represents the number of neurons at the ith position where i is the index of the tuple.\n",
        "* length of the tuple denotes the total number of hidden layers in the network\n",
        "\n"
      ],
      "metadata": {
        "id": "rqkbRbYZMRfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP_clf = MLPClassifier(\n",
        "    hidden_layer_sizes = (15, 10, 5) # 15 neuron in 1st layer, 10 in 2nd and 5 in 3rd\n",
        ")"
      ],
      "metadata": {
        "id": "PtogPw7-MQoL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLPClassifier\n",
        "> how to perform regularization in MLPClassifier?\n",
        "* `alpha` parameter sets L2 penalty. default alpha = 0.0001\n",
        "\n",
        "> how to set the activation function for hidden layers?\n",
        "* indentity -> f(x) = x\n",
        "* logistic sigmoid -> f(x) = 1/(1+exp(-x))\n",
        "* tanh -> f(x) = tanh(x)\n",
        "* relu [default] -> f(x) = max(0, x)\n",
        "\n",
        "> how to perform weight optimization in MLPClassifier?\n",
        "optimizes the log-loss function using LBFGS or stochastic gradient descent.\n",
        "* solver ->\n",
        "    * lbfgs -> classifier will not use minibatch, size of minibatches can be set to other stochastic optimizers using `batch_size`. default auto.\n",
        "    ```py\n",
        "    batch_size = min(200, n_samples)\n",
        "    ```\n",
        "    * sgd\n",
        "    * adam(default)\n",
        "\n",
        "> how to view weight matrix coefficients of trained MLPClassifier?\n",
        "\n",
        "coefs_\n",
        "```py\n",
        "# weights between input and first hidden layer\n",
        "print(MLP_clf.coefs_[0])\n",
        "```\n",
        "\n",
        "> how to view bias vector of trained MLPClassifier?\n",
        "\n",
        "intercepts_\n",
        "```py\n",
        "# bias for first hidden layer\n",
        "print(MLP_clf.coefs_[0])\n",
        "```\n",
        "\n",
        "* learning_rate ->\n",
        "    * constant (default)\n",
        "    * invscaling\n",
        "    * adaptive\n",
        "\n",
        "* learning_rate_init -> 0.001\n",
        "* power_t -> 0.5\n",
        "* mat_iter -> 500\n",
        "\n",
        "> * learning_rate and power_t are used only for sgd\n",
        "> * learning_rate_init for sgd or adam\n",
        "> * shuffle is used to shuffle samples in each iteration where solver is sgd or adam\n",
        "> * momentum is used for gradient descent update when solver is sgd\n",
        "\n",
        "### MLPRegressor\n",
        "trains using backpropogation with no activation function in the output layer. therefore it uses the square error as the loss function and the output is set of continuous values.\n",
        "parameters are same as of MLPClassifier"
      ],
      "metadata": {
        "id": "pfFZELb0NFJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "MLP_reg = MLPRegressor()\n",
        "MLP_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "zkOeb5neNESu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLP_reg.predict(X_test) # gives labels for new samples\n",
        "MLP_reg.score(X_test, y_test) # gives r2 score"
      ],
      "metadata": {
        "id": "MO7yvEEIe0pz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}