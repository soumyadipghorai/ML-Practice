{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "sklearn uses an otpimized version of the `CART` algo, however it doesn't support categorical variables for now.\n",
    "\n",
    "* **splitter** -> strategy for splitting at each node. (best, random ==> default = best)\n",
    "* **max_depth** -> max depth of the tree. When `None` the tree expanded until all leaves are pure or they contain less than `min_sample_split` samples. (int)\n",
    "* **min_sample_split** -> min samples required to split an internal node. (int, float ==> default = 2)\n",
    "* **min_sample_leaf** -> min number of samples required to be at a leaf node. (int, float ==> default = 1)\n",
    "* **Criterion** -> specifies function to measure the quality of a split. \n",
    "    * classification -> gini(default), entropy\n",
    "    * regression -> squarred_error(default), friedman_mse, absolute_error, poisson\n",
    "\n",
    "### Visualization : \n",
    "`sklearn.tree.plot_tree` \n",
    "* decision_tree -> tree to be plotted \n",
    "* max_depth -> max depth of the presentation. if None the tree is fully generated \n",
    "* feature_names -> names of each of the features. (default = None)\n",
    "* class_names -> Names of each of the target classes in ascending numerical order. (None)\n",
    "* label -> whether to show informative labels for impurity (None)\n",
    "\n",
    "#### How to avoid overfitting : \n",
    "* Pre-pruning -> use hyper-parameter seach for finding the best set of params. \n",
    "* Post-pruning ->  first grows tree without any constraints and the uses `cost_complexity_pruning` with `max_depth` and `min_sample_split`\n",
    "\n",
    "> ### Tips\n",
    "> * decision tree tend to overfit data with large number of features, make sure that we have the right ratio of samples to number of features. \n",
    "> * perform dimensionality reduction (PCA, or feature selection) on a data before using it for trainng the tree. it gives better chance of finding discriminative features.\n",
    "> * visualize the trained tree with `max_depth` = 3 as an initial tree depth to get a feel for the fitment and then increase the depth.\n",
    "> * balance the dataset before training to prevant the tree from being biased towards the classes that are dominant \n",
    "> * use `min_sample_split` or `min_sample_leaf` to ensure that multiple samples influence wvery decision in the tree, by controlling which splits will be considered. \n",
    ">    * a very small number will usually mean the tree will overfit. \n",
    ">     * a large number will prevent the tree from learning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
