{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dummy Regressor` helps in creating a baseline model for regression. <br>\n",
    "we can either set it to `mean`, `median`, `quantile` (can be specified) or  `constant` (can be specified)\n",
    "\n",
    "* LinearRegression() uses normal equation \n",
    "* SGDRegressor() uses iterative optimization --> uses stochastic gradient descent and used for large data (>10k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDRegressor \n",
    "#### hyperparameter\n",
    "* **Loss :** it has two types of loss as hyperparameter. `squared error` and `huber` (used tyo make linear regressor robust to the outliers).\n",
    "* **Penalty :** it has different penalty like `l1`, `l2`, `elasticnet`.\n",
    "* **Learning rate :** learning rate can be `constant`, `optimal`, `invscaling` and `adaptive`. By default it uses invscaling.\n",
    "* **Early stopping** : stopping criteria or `early_stopping` can be True or False.\n",
    "\n",
    "#### Feature scaling for SGDRegressor \n",
    "SGDRegressor is sensitive to feature scaling. \n",
    "\n",
    "> * Feature scaling is not needed for word freq and indicator features as they have intrinsic scale. \n",
    "> * Feature extracted using PCA should be scaled by some constant c such that the avg L2 norm of the training data equals one.\n",
    "\n",
    "#### Shufle \n",
    "`SGDRegressor(shuffle = True)` will shuffle the training data after each epoch. default True \n",
    "\n",
    "##### WHY?\n",
    "In Stochastic Gradient Descent (SGD) and its variants like `SGDRegressor`, shuffling the training data after each epoch is recommended for several important reasons:\n",
    "\n",
    "1. **Avoiding Bias**: When training a machine learning model, the order in which you present training examples to the model can have an impact on the model's performance. If you feed the examples to the model in a specific order, it may start to learn and adapt primarily to the patterns in that order, which can introduce bias into the model. Shuffling the data helps break any potential patterns or biases in the order of examples.\n",
    "\n",
    "2. **Promoting Generalization**: Shuffling the training data at the beginning of each epoch ensures that the model is exposed to a diverse set of examples in each epoch. This diversity helps the model generalize better because it prevents the model from memorizing the specific order of the examples. Generalization is crucial for the model's ability to make accurate predictions on new, unseen data.\n",
    "\n",
    "3. **Converging Faster**: Shuffling the data introduces randomness into the training process. This randomness can help the optimization algorithm escape local minima and converge faster to a global minimum of the loss function. Without shuffling, the optimizer might get stuck in a suboptimal solution.\n",
    "\n",
    "4. **Reducing Overfitting**: By shuffling the data, you make it less likely that the model will overfit to specific patterns or outliers in the training data. Overfitting occurs when a model becomes too specialized in capturing the noise or idiosyncrasies of the training data, which results in poor generalization to new data.\n",
    "\n",
    "5. **Ensuring Fairness**: In some cases, the data might be ordered in a way that represents a particular class distribution or pattern that changes over time. Shuffling the data ensures that each class or pattern has an equal chance to be represented in each epoch, which can be important for fairness and balanced learning.\n",
    "\n",
    "##### Learning Rate : \n",
    "\n",
    "\n",
    "##### invscaling : \n",
    "\n",
    "* default setting for learning rate is `invscaling` and we use `eta0 = 1e-2` and `power_t = 0.25` which can be optimized to speed up or slow down the learning process \n",
    "\n",
    "* SGDRegressor uses different learning rate in different iterations. \n",
    "Learning rate reduces after every itertion and the learning rate is defined by `eta = eta0/pow(t, power_t)` t is the current iteration. \n",
    "\n",
    "```py\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Set the initial learning rate (eta0) and power_t\n",
    "sgd_regressor = SGDRegressor(eta0=0.01, power_t=0.5, max_iter=1000, tol=1e-3)\n",
    "\n",
    "# Fit the model to your training data\n",
    "sgd_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions or evaluate the model\n",
    "predictions = sgd_regressor.predict(X_test)\n",
    "\n",
    "```\n",
    "`eta` is the current learning rate for the particular iteration. for t = 1 `eta = eta0`\n",
    "\n",
    "`eta0` is the initial learning rate which remains constant. \n",
    "* speed up : initialize with a large value \n",
    "* slow down : inititalize with a small value. \n",
    "\n",
    "power_t is the power of t. t = 1, 2, 3, ... \n",
    "* speed up : initialize with a small value \n",
    "* slow down : initialize with a large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.009330329915368075\n",
      "0.008959584598407623\n",
      "0.008705505632961241\n",
      "0.008513399225207847\n",
      "0.008359588020779369\n",
      "0.008231712539930443\n",
      "0.008122523963562354\n",
      "0.008027415617602307\n",
      "0.007943282347242816\n"
     ]
    }
   ],
   "source": [
    "# speed up training \n",
    "initial_learning_rate = 0.01 # et0 \n",
    "powert_t = 0.1\n",
    "\n",
    "for i in range(10) : \n",
    "    eta = (initial_learning_rate/ pow(i+1, powert_t))\n",
    "    print(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.005946035575013606\n",
      "0.004386913376508308\n",
      "0.0035355339059327372\n",
      "0.002990697562442441\n",
      "0.0026084743001221454\n",
      "0.002323680802425408\n",
      "0.0021022410381342864\n",
      "0.0019245008972987524\n",
      "0.0017782794100389228\n"
     ]
    }
   ],
   "source": [
    "# slow down training \n",
    "initial_learning_rate = 0.01 # et0 \n",
    "powert_t = 0.75\n",
    "\n",
    "for i in range(10) : \n",
    "    eta = (initial_learning_rate/ pow(i+1, powert_t))\n",
    "    print(eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### constant : \n",
    "keeps the learning rate constant through out all iterations \n",
    "##### adaptive : \n",
    "* learning rate to kept to initial value as long as the loss keep decreasing.\n",
    "* once the stopping criteria is hit learning rate is divided by 5 and the training loop continues.\n",
    "* algo stops when learning rate goes below 10^-6\n",
    "\n",
    "#### Max iterations \n",
    "* `max_itr` is number of epochs. By default its 1000. \n",
    "\n",
    "> **Practical Tip** : \n",
    "> Its observed that SGD converges after observing approx 10^6 training samples. <br>\n",
    "> so the best guess for `max_itr` = np.ceil(10^6/n), where n is the number of samples in training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasonable guess for max_itr for 100 samples --> 10000.0\n",
      "reasonable guess for max_itr for 1000 samples --> 1000.0\n",
      "reasonable guess for max_itr for 10000 samples --> 100.0\n",
      "reasonable guess for max_itr for 100000 samples --> 10.0\n",
      "reasonable guess for max_itr for 1000000 samples --> 1.0\n",
      "reasonable guess for max_itr for 10000000 samples --> 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "for i in range(6) : \n",
    "    trainig_data_len = 100 * pow(10, i)\n",
    "    max_itr = np.ceil(pow(10, 6)/trainig_data_len)\n",
    "    print(f'reasonable guess for max_itr for {trainig_data_len} samples --> {max_itr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to set stopping criteria in SGDRegressor? \n",
    "\n",
    "* Option 1 : `tol = 1e-3`, `n_iter_no_change = 5`, `max_iter = 500` it stops \n",
    "    * when the training loss does  n't improve (loss > best_loss - tol) for n_iter_no_change consecutive epochs \n",
    "    * else after a max num of iteration `max_iter`\n",
    "\n",
    "* Option 2 : `early_stoppin = True`, `validation_fraction = 0.2` along with the prev parameters, it stops \n",
    "    * when validation_score doesn't improve by at least tol for n_iter_no_change consecutive epochs \n",
    "    * else after a max num of iteration `max_iter` \n",
    "\n",
    "### Avgeraged SGD\n",
    "updating weight just based on the last sample weight can be a bit unstable so we can take avg of previous weights to make it more stable and it helps avoiding the local minima trap. \n",
    "\n",
    "* `average = True` gives the avg weights from prev updates \n",
    "* `average = 10` gives the avg weights from prev updates starting after 10 samples, works best with large num of features and high value of `eta0` \n",
    "\n",
    "sometimes we do multiple runs of SGD and we want to use the value of the prev weight vector as initialization for the next SGD run. `warm_start = True` does the same. By default its Flase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import  mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "sgd_reg = SGDRegressor(\n",
    "    max_iter= 1, tol = -np.infty, warm_start= True, \n",
    "    penalty = None, learning_rate = 'constant', eta0 = 0.0005\n",
    ")\n",
    "\n",
    "error = []\n",
    "epoch_ = [i+1 for i in range(1000)]\n",
    "for epoch in range(1000) : \n",
    "    # in each iteration the we use the value of weight from the prev itr to start with \n",
    "    sgd_reg.fit(X_train, y_train) \n",
    "    y_val_predict = sgd_reg.predict(X_val)\n",
    "    val_error = mean_absolute_error(y_val, y_val_predict)\n",
    "    error.append(val_error)\n",
    "\n",
    "sns.lineplot(x = epoch_, y = error);\n",
    "plt.xlabel('epoch');\n",
    "plt.ylabel('error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation : \n",
    "`.coef_` gives coefficients of regression and `.intercept_` gives the intercept and this works for all regression methods \n",
    "\n",
    "* training error or empirical error \n",
    "* test error or generalization error \n",
    "\n",
    "`lr.score(x_test, y_test)` gives the R2 score \n",
    "\n",
    "common evaluation matrix \n",
    "* `mean_absolute_error` \n",
    "* `mean_squarred_error` \n",
    "* `r2_score` \n",
    "\n",
    "special evaluation matrix \n",
    "* `mean_squared_log_error` useful for targets with exponential growths like population --> penalizes under-estimation heavier than the over-estimation\n",
    "* `mean_absolute_percentage_error` --> sensitive to relative error \n",
    "* `median_absolute_error` --> robust to outliers\n",
    "* `max_error` --> worst case error (only used in single output regression, doesn't support multi-output regression)\n",
    "\n",
    "> * For scores higher is better \n",
    "> * For error lower is better \n",
    "\n",
    "to convert error metric to score metric use `neg_` as suffix\n",
    "\n",
    "\n",
    "### cross validation \n",
    "In case, we get comparable performance on train and test with a specific split, is this performance guranteed on other splits???\n",
    "\n",
    "* If the test set is small then test error obtained is unstable and would not reflect the true error on larger test set \n",
    "* if only easier samples are kept aside as test set then this would lead to optimistic estimation of the true test error \n",
    "\n",
    "To avoid this we use cross validation. It creates multiple train and test sets and train on different sets and this way we ensure robust performance evaluation.\n",
    "\n",
    "* KFold --> k-1 for training and 1 for test \n",
    "```\n",
    "KFold(n_splits=2, random_state=None, shuffle=False)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "TRAIN: [2 3] TEST: [0 1]\n",
    "TRAIN: [0 1] TEST: [2 3]\n",
    "```\n",
    "\n",
    "* RepeatedKfold --> repeats the KFold \n",
    "* LeaveOneOut --> will leave exactly one out as test \n",
    "* ShuffleSplit --> Shuffles the training data before splitting --> robust for class distribution so even if the class distributions are not uniform it works well \n",
    "\n",
    "```\n",
    "ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n",
    "for train_index, test_index in rs.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "TRAIN: [1 3 0 4] TEST: [5 2]\n",
    "TRAIN: [4 0 2 5] TEST: [1 3]\n",
    "TRAIN: [1 2 4 0] TEST: [3 5]\n",
    "TRAIN: [3 4 1 0] TEST: [5 2]\n",
    "TRAIN: [3 5 1 0] TEST: [2 4]\n",
    "```\n",
    "\n",
    "for different scoring methods we use `cross_validate` class and pass different `scoring` parameters to it and you can also pass a list of scores to be considered. \n",
    "\n",
    "\n",
    "### Learning curve \n",
    "helps us to study the effect of #samples on training and test errors. we can plot this and check.\n",
    "\n",
    "### Overfitting/Underfitting \n",
    "1. fit linear model with diff num of features \n",
    "2. for each model, obtain training and test errors \n",
    "3. plot #features vs error graph - one each for training and test error \n",
    "4. examine the graphs to detect underfitting/overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4ba08aa27a50dfc4411e691f4a22a11e700bce603ba8e16cecc3bc830c9da89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
